# Contributor Guide

## Reports

- [Pylint](pylint)
- [MyPy](mypy)
- [Coverage](coverage)

## Getting started

We use Docker as a clean, reproducible development environment within which to build, test, generate docs, and so on. See the [Setup Development Environment](#setup-development-environment) to get that working. Running things natively isn't a supported/maintained thing.

Once you've got Docker setup and built the `cicd` image, you should be able to run the full test suite, as discussed [below](#tests). You should also be able to build the documentation, also discussed [further down](#documentation).

## Requirements

_TL;DR: Run `requirements/update.sh` to update requirements._

### Package Requirements

One stated design goal for `tubthumper` is to have no external dependencies, i.e. stdlib only. However, in order to support some modern Python features in older versions, we DO have a few conditional dependencies, which can be found in the `install_requires` section of `setup.cfg`.

### Test Requirements

In addition to conditional dependencies to run on older versions of Python, `tubthumper` also requires additional packages to run unit tests. These can be found in `requirements/test_requirements.txt`, an auto-generated file produced from `requirements/test_requirements.in`. These are separated from the general requirements to allow `tox` to install just these in the various virtual environments it creates. See [below](#unit-tests-with-tox) for more info on `tox`.

### Docs Requirements

Building the documentation for `tubthumper` requires additional packages. These can be found in `requirements/docs_requirements.txt`, an auto-generated file produced from `requirements/docs_requirements.in`. These are separated from the general requirements to allow Read the Docs to install them in their build process. See [below](#publishing-documentation) for more info on Read the Docs.

### Dev Requirements

There are two requirements files associated with the `cicd` Docker image:
1) `requirements/requirements.in`
2) `requirements/requirements.txt`

The `.in` file is where we collect immediate dependencies, described in PyPI format (with versions pinned only as needed). The `.txt` file is generated by running the `requirements/update.sh` shell script, which uses `pip-compile`. While using [hashes](https://pip.pypa.io/en/stable/cli/pip_install/#hash-checking-mode) would be nice, different platforms, e.g. Apple M1's ARM vs Intel's x86, sometimes require different wheels with different hashes. This is true despite ensuring a consistent Linux OS in Docker sadly. In the spirit of enabling a diverse ecosystem of developers with different machines, I've kept hashing off.

This gives us both a flexible way to describe dependencies while still achieving reproducible builds. Inspired by [this](https://hynek.me/articles/python-app-deps-2018/) and [this](https://pythonspeed.com/articles/pipenv-docker/).

## Setup Development Environment

_TL;DR: To build the `cicd` Docker image, run `cicd/setup.sh`._

The `setup.sh` shell script in the `cicd` directory will build the `cicd` image for you, installing both OS-level and Python dependencies, while also installing the `tubthumper` package in [editable mode](https://pip.pypa.io/en/stable/cli/pip_install/#install-editable). Different workflows will mount the relevant parts of the repo in a Docker container, allowing fast development, i.e. no need to re-build the image to test/document/etc. your changes as you go.

You'll need to build this image for each commit (the image is tagged by git commit SHA), but this is generally quite fast because of a few cacheing tricks.

### Cacheing Tricks

We do a couple of neat cacheing tricks to speed things up. First off, in the `Dockerfile`s themselves, we use the `RUN --mount=type=cache` functionality of Docker BuildKit to cache apt packages stored in `/var/cache/apt` and Python packages stored in `~/.cache/pip`. This keeps your local machine from re-downloading new packages each time. h/t Itamar Turner-Trauring from his site [pythonspeed](https://pythonspeed.com/articles/docker-cache-pip-downloads/) for inspiration.

Second, we use the new `BUILDKIT_INLINE_CACHE` feature to cache our images using Docker Hub. This is configured in the `docker build` command, and is smart enough to only download the layers you need. This DOES work in Github Actions, while the prior functionality does not. h/t Itamar Turner-Trauring from his site [pythonspeed](https://pythonspeed.com/articles/speeding-up-docker-ci/) for inspiration.

## Tests

_TL;DR: Run `cicd/test.sh --all` to run the full suite of tests._

### Version Check

_TL;DR: Run `test/version.sh to confirm version is valid._

We follow [PEP 440](https://www.python.org/dev/peps/pep-0440/) for semantic versioning syntax. The `test/version.sh` shell script checks the current version, ensuring that it supercedes the [best common ancestor](https://git-scm.com/docs/git-merge-base#_description) the current branch shares with the `main` branch. This ensures that the resulting history of the `main` branch has a version that is strictly increasing.

When starting a new feature branch, you'll want to increment the version, likely by incrementing the `.devN` number, in order to pass this test.

### Requirements Check

_TL;DR: Run `test/requirements.sh to confirm requirements are up-to-date._

As described [above](#requirements), we auto-generate the `requirements.txt` file used to pin Python dependencies in the `cicd` Docker image. The `test/requirements.sh` shell script ensures that any changes to the files associated with updating requirements have been propagated to `requirements.txt`.

If this test is failing, see the [requirements](#requirements) section above to remedy the issue.

### Black Code Formatting

_TL;DR: Run `test/black.sh` to format your code._

We use [Black](https://black.readthedocs.io/en/stable/index.html) for code formatting. To format your code, run the `test/black.sh` shell script to get all your spaces in a row. Black configuration can be found in the `pyproject.toml` file at the root of the repo.

Black is setup to discover Python files recursively from the root of the repo, ignoring files and directories matching any `.gitignore` files.

### isort Import Ordering

_TL;DR: Run `test/isort.sh` to order your imports._

For import ordering, we use [isort](https://pycqa.github.io/isort/). To get imports ordered correctly, run the `test/isort.sh` shell script. isort configuration can be found in the `pyproject.toml` file at the root of the repo.

isort is setup to discover Python files recursively from the root of the repo, ignoring files and directories matching any `.gitignore` files.

### Pylint Code Linting

_TL;DR: Run `test/pylint.sh` to lint your code._

We use [Pyint](https://pylint.pycqa.org/en/latest/) for Python linting (h/t Itamar Turner-Trauring from his site [pythonspeed](https://pythonspeed.com/articles/pylint/) for inspiration). To lint your code, run the `test/pylint.sh` shell script. In addition to showing any linting errors, it will also print out a report, which is also saved as `reports/pylint.txt` for ease of reference. A `unit_test1.stats` file will also be generated in the root of the repo, which pylint uses to store previous results (the generated report shows differences between the last run). Pylint configuration can be found in the `pylintrc` file at the root of the repo.

Pylint is setup to lint the `tubthumper` & `test/unit_tests` packages along with the `version/inner_check.py`, `docs/source/conf.py`, & `publish/tag.py` modules. To add more modules or packages for linting, edit `test/pylint.sh`.

### Shellcheck Shell Script Linting

_TL;DR: Run `test/shellcheck.sh` to lint your shell scripts._

We use [ShellCheck](https://www.shellcheck.net/) for shell script linting (h/t [Julia Evans](https://wizardzines.com/comics/shellcheck/) for introducing me to shellcheck). To lint your shell scripts, run the `test/shellcheck.sh` shell script (yes, I know). There is no Shellcheck configuration.

Shellcheck is setup to run on all files tracked by git that end `.sh`. This can be edited in `test/inner_shellcheck.sh`.

### Mypy Static Type Checking

_TL;DR: Run `test/mypy.sh` to type check your code._

We use [Mypy](https://mypy.readthedocs.io/en/stable/) for static type checking. To type check your code, run the `test/mypy.sh` shell script. In addition to printing any type checking errors, Mypy will also generate an html report, which can be found in `reports/mypy/index.html` for viewing in your browser.

Mypy will run faster on subsequent runs since mypy caches results in a `.mypy_cache` directory in the root of the repo. Mypy configuration can be found in the `pyproject.toml` file at the root of the repo.

Mypy is setup to run on the `tubthumper` package. To add more modules or packages for type checking, edit `test/mypy.sh`.

### Unit Tests

_TL;DR: Run `test/unit_tests.sh` to unit test your code._

We use [unittest](https://docs.python.org/3/library/unittest.html) for unit testing. To unit test your code, run the `test/unit_tests.sh` shell script. This will run unit tests in Python 3.8 only.

unittest is setup to discover tests using its [test discovery](https://docs.python.org/3/library/unittest.html#test-discovery) functionality from the `test` directory. In practice, we put all unit tests in the `test/unit_tests` package directory. All test files must match the pattern `test*.py`. To add more directories for unit test discovery, edit `test/inner_unit_tests.sh`.

### Unit Tests with tox

_TL;DR: Run `test/unit_tests.sh --tox` to run the full suite of unit tests._

In addition to supporting fast unit testing, we use [tox](https://tox.readthedocs.io/en/latest/config.html) for unit testing across all supported versions of Python. To run the full unit test suite, run the `test/unit_tests.sh` shell script with the `--tox` flag.

If you haven't run this before, the script will initialize tox, creating the `.tox` directory in the root of the repo with files associated with each Python environment. This step is skipped on subsequent runs, making it MUCH faster. tox configuration can be found in the `pyproject.toml` file at the root of the repo.

tox is setup to run the same unit tests as [described above](#unit-tests).

### Test Coverage

Tox is also setup to confirm 100% test coverage using [Coverage.py](https://coverage.readthedocs.io/en/coverage-5.5/). In addition to printing any coverage gaps from the full test suite, Coverage.py will also generate an html report, which can be found in `reports/coverage/index.html` for viewing in your browser.

Coverage.py configuration can be found in the `pyproject.toml` file at the root of the repo.

### Packaging Tests

_TL;DR: Run `test/packaging.sh` to test the package build._

We use [`build`](https://pypa-build.readthedocs.io/en/latest/) to build source distributions and wheels. We then use [`check-wheel-contents`](https://github.com/jwodder/check-wheel-contents) to test for common errors and mistakes found when building Python wheels. Finally, we use [`twine check`](https://twine.readthedocs.io/en/latest/#twine-check) to check whether or not `tubthumper`'s long description will render correctly on PyPI. To test the package build, run the `test/packaging.sh` shell script. While there is no configuration for `build` or `twine`, the configuration for `check-wheel-contents` can be found in the `pyproject.toml` file at the root of the repo.

### Documentation Tests

See [below](#documentation) for more info on the documentation build process. The test script simply builds the docs, which are configured to error out if any warnings are found.

## Documentation

_TL;DR: To build the documentation website, run `docs/build.sh`._

We use [Sphinx](https://www.sphinx-doc.org/en/master/index.html) for documentation site generation. To build the documentation website, run the `docs/build.sh` shell script. To view it, open `docs/build/html/index.html` in your browser.

Sphinx will only generate pages that have changed since your last build, but isn't perfect at this determination, so you may need to clear out your `docs/build` directory to start fresh. Sphinx configuration can be found in `docs/source/conf.py`.

Sphinx is setup to generate pages based on what it finds in the `toctree` directive in `docs/source/index.md`. To add new pages, add them to the table of contents with that directive.

### API Reference

The "API Reference" page is mostly auto-generated using the [`autodoc`](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html), [`autosummary`](https://www.sphinx-doc.org/en/master/usage/extensions/autosummary.html), [`intersphinx`](https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html), and [`linkcode`](https://www.sphinx-doc.org/en/master/usage/extensions/viewcode.html) Sphinx extensions. Classes, functions, decorators, and so on need to be added manually to the `docs/source/api.rst` file, but the entries are auto-populated using type annotations and docstrings.

### Changelog

We document changes in the `docs/source/CHANGELOG.md` file. This project adheres to the [keep a changelog](https://keepachangelog.com/en/1.0.0/) standard. Before committing changes that impact users, make sure to document features added, changed, deprecated, removed, fixed, or security-related changes to the "## Unreleased" section.

## Releasing

### Prereleases vs Final Releases

In order to keep a monotonically increasing version in the git history of the `main` branch, prereleases, e.g. alpha, beta, or release-candidates, should never be merged back into the `main` branch.

The type of final release (major, minor, or micro) should be determined by the types of unreleased changes in the changelog. Any "removed" changes call for a major release. "Added" changes call for a minor release. Otherwise, a "micro" release is called for.

Final releases should also be accompanied by an update to the changelog, renaming the "## Unreleased" section to "## {major}.{minor}.{micro} (YYYY-MM-DD)"

### Publishing the Package to PyPI

_TL;DR: To publish the package to PyPI, run `publish/package.sh --repository pypi`._

We use [`build`](https://pypa-build.readthedocs.io/en/latest/) to build source distributions and wheels, which we publish to [PyPI](https://pypi.org/) using [`twine`](https://twine.readthedocs.io/en/latest/). This is all handled using the `publish/package.sh` shell script. This script requires two environment variables are defined: `TWINE_USERNAME` and `TWINE_PASSWORD`, which are used for authenticating to the PyPI server.

To test out package publishing, you can run `publish/package.sh testpypi` to publish to [TestPyPI](https://packaging.python.org/guides/using-testpypi/).

### Publishing Documentation

We use [Read the Docs](https://docs.readthedocs.io/en/stable/index.html) for building and publishing `tubthumper`'s documentation. Its Github integration makes this process seamless. Read the Docs configuration can be found in the `.readthedocs.yaml` file at the root of the repo.

While documentation for the `tubthumper` package is generated and hosted by Read the Docs, the documentation can be found at a custom domain: tubthumper.mattefay.com. You can read more about this [here](https://docs.readthedocs.io/en/stable/custom_domains.html).

### Publishing a Tag to Github

_TL;DR: To publish a tag to Github, run `cicd/tag.sh`._

All releases, including prereleases, get a tag published to github using Github's [Create a release](https://docs.github.com/en/rest/reference/repos#create-a-release) REST API. This can be accomplished using the `cicd/tag.sh` shell script. This script requires one environment variable: `GITHUB_TOKEN`, which is used to authenticate.

This script grabs the current version, and also collects up change information from the changelog.

## Continuous Integration & Continuous Deployment

We use Github actions to run our CI/CD pipeline on every pull request. The configuration can be found in `.github/workflows/cicd.yaml`. That said, every step of every job can also be run locally.

### Main

This is the "main" job, which consists of setting up, running the test suite, pushing to docker hub, and publishing the package. Each step is described below.s

#### Setup

See [above](#setup-development-environment) for more info on this step.

#### Test

See [above](#tests) for more info on testing.

#### Push to Docker Hub

_TL;DR: To push Docker images to Docker Hub, run `cicd/push.sh`._

Docker images are pushed when a build is triggered from the `main` branch. To run this locally, you'll need to be logged in to Docker. We use the `docker/login-action@v1` build action to login on Github Actions, and it uses a personal access token named `github-actions` from my Docker Hub account to do that, with the username and token stored as secrets. This uses the `matteosox/tubthumper-cicd` repository.

In addition to pushing the image tagged with the git SHA of the code producing it, we also push an untagged (i.e. tagged as `latest`) image.

#### Tag on Github

For prereleases or final releases triggered from the `main` branch, we publish a tag to Github. See [above](#publishing-a-tag-to-github) for more info.

#### Publish to PyPI

For workflows triggered from the `main` branch, we use the `cicd/publish.sh` shell script in CI, which wraps `publish/package.sh` with an additional check to ensure we only publish final releases. See [above](#publishing-the-package-to-pypi) for more info on publishing the package.

#### Github Actions Artifacts

We store an artifact of the `reports` directory at the completion of each run, making debugging failed runs easier. You can read more about Github Actions artifacts feature [here](https://docs.github.com/en/actions/advanced-guides/storing-workflow-data-as-artifacts).

### OS Compatibility

We use [`tox-gh-actions`](https://opensourcelibs.com/lib/tox-gh-actions) to configure tox to run on multiple versions of Python in a separate job intended to test compatibility across different operating systems. Using Github Actions' [build matrix feature](https://docs.github.com/en/actions/learn-github-actions/managing-complex-workflows#using-a-build-matrix), we're able to run unit tests on MacOS, Windows, & Linux, for each supported version of Python.

#### Codecov

In addition to validating that test coverage is 100% as part of the "main" CI/CD workflow — discussed further in the [test coverage](#test-coverage) section — we also upload coverage reports to codecov.io. This provides nice pull request comments and annotation, in addition to a fancy badge.

## Pull Requests

The `main` branch has [branch protections](https://help.github.com/en/github/administering-a-repository/about-protected-branches) turned on in Github, requiring one reviewer to approve a PR before merging. We also use the code owners feature to specify who can approve certain PRs. As well, merging a PR requires status checks (Read the Docs and both CI/CD jobs) to complete successfully.

When naming a branch, please use the syntax `firstname/branch-name-here`. If you plan to collaborate with others on that branch, use `team/branch-name-here`.

## Future Work

- Update type annotations to use types.ParamSpec once Mypy supports them (currently a new feature in Python 3.10). See [here](https://github.com/python/mypy/issues/8645).
- Remove python3.10-distutils once pip migrates from `distutils` to `sysconfig`. See [here](https://pip.pypa.io/en/stable/news/#id60) & [here](https://docs.python.org/3.10/library/distutils.html#module-distutils).
- Add py310 to black target list once black supports py310
